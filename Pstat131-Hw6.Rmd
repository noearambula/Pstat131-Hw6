---
title: "Pstat131 Hw6"
author: "Noe Arambula"
date: '2022-05-25'
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---

# Loading Packages

```{r message=FALSE, warning=FALSE}
library(yardstick)
library(tidyverse)
library(tidymodels)
library(ISLR)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
library(corrr)
library(corrplot)
library(ranger)

tidymodels_prefer()
```

# Set Data and Seed

```{r}
pokemon <- read_csv("Pokemon.csv")

set.seed(777)
```

# Question 1

Read in the data and set things up as in Homework 5:

-   Use `clean_names()`

-   Filter out the rarer PokÃ©mon types

-   Convert `type_1` and `legendary` to factors

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

-   Dummy-code `legendary` and `generation`;

-   Center and scale all predictors.

    ## Setup Like Hw 5

```{r}

clean_pokemon <- clean_names(pokemon)

# Filter entire dataset
filtered_pokemon <- filter(clean_pokemon, type_1 == "Bug" | type_1 == "Fire" 
                          | type_1 == "Grass" | type_1== "Normal" | 
                          type_1 == "Water" | type_1 == "Psychic")

# Convert type_1 and legendary to factors
filtered_pokemon$type_1 <- factor(filtered_pokemon$type_1)
filtered_pokemon$legendary <- factor(filtered_pokemon$legendary)

```

## Initial Split, Vfold, and Recipe Setup

```{r}
set.seed(777)

# Initial split
pokemon_split <- initial_split(filtered_pokemon, strata = type_1, prop = 0.8)

pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

# Folding Training Data
set.seed(777)
pokemon_fold <- vfold_cv(pokemon_train, v = 5, strata = type_1)

# Recipe
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack 
                         + speed + defense + hp + sp_def, data = pokemon_train) %>%
  step_dummy(all_nominal_predictors()) %>%   # creates dummy variables
  step_normalize(all_predictors())    # Centers and Scales all variables

pokemon_recipe
```

# Question 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

What relationships, if any, do you notice? Do these relationships make sense to you?

```{r}
# note for a correlation matrix variable must be numeric
pokemon_train %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot(type = 'lower',
           method = 'number')

```

I chose to leave all continuous variables in since I feel it would be important to see all relationships. I see a strong relationship between generation and number which makes complete sense since pokemon that were created later are part of the next generation accordingly. There are also relationships between all of a Pokemon's stats and the total which also makes sense since the stats are what makeup the total number. Special defense and defense seem to be slightly correlated as well.

# Question 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`.

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r}
# general decision tree specification
tree_spec <- decision_tree() %>%
  set_engine("rpart")

# classificaition decision tree engine/model
class_tree_spec <- tree_spec %>%
  set_mode("classification")

# Workflow tuning cost complexity
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(pokemon_recipe)

# setup grid
set.seed(777)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf, 
  resamples = pokemon_fold, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
```

```{r warning=FALSE}
autoplot(tune_res)
```

The lower the complexity parameter the higher and better roc_auc we get. It seems that the 6th model performs the best with cost complexity parameter about 0.02

# Question 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}

# this will also show the best model based on roc_auc but does not show the actual roc_auc number
best_model <- select_best(tune_res, metric = "roc_auc")
best_model

# shows all models and organizes from worst to best roc_auc
collect_metrics(tune_res) %>%
  arrange(mean)
```

The roc_auc of my best-performing pruned decision tree on the folds is 0.6535274

# Question 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
#fit model
best_model <- select_best(tune_res)

class_tree_final <- finalize_workflow(class_tree_wf, best_model)

class_tree_final_fit <- fit(class_tree_final, data = pokemon_train)

# visualize
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

# Question 5 (continued)

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r}
# Random forest model and workflow
rf_mod <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_recipe(pokemon_recipe) %>%
  add_model(rf_mod)

# grid
param_grid <- grid_regular(mtry(range = c(2, 6)), 
                     trees(range = c(2, 5)), 
                     min_n(), levels = c(8,8,8)) # I am not sure what a good range for min_n would be

```

Mtry is the number of predictors that will be randomly chosen at each split of the tree models thus it cannot be lower than 1 since then no predictors would be chosen or greater than 8 since we only have 8 predictors. If mtry = 8 that would represent a bagging model.

Trees represents the number of trees that will be used in each model

min_n represents the minimum number of of data points needed at each node/tree end that is needed in order to split it further into another tree

# Question 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r}
tune_res <- tune_grid(
  rf_wf, 
  resamples = pokemon_fold, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
```

# Question 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}

```

# Question 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

```{r}

```

# Question 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results.

What do you observe?

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}

```

# Question 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set.

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?

```{r}

```
